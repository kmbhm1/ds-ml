{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edbd4df6-8af2-48d5-bdf2-7994a62567ed",
   "metadata": {},
   "source": [
    "# Markov Chain\n",
    "\n",
    "## Purpose\n",
    "\n",
    "The purpose of this notbook is to investigate the Markov Chain algorithm, focused on use as a generative model.\n",
    "\n",
    "## References\n",
    "\n",
    "- Project Inspiration: [Nazarko](https://towardsdatascience.com/text-generation-gpt-2-lstm-markov-chain-9ea371820e1e), [Rizvi](https://medium.com/analytics-vidhya/a-comprehensive-guide-to-build-your-own-language-model-in-python-5141b3917d6d)\n",
    "- Markov Chain Explanations: [Powell](https://setosa.io/ev/markov-chains/), [Soni](https://towardsdatascience.com/introduction-to-markov-chains-50da3645a50d), [Strika](https://towardsdatascience.com/markov-chains-how-to-train-text-generation-to-write-like-george-r-r-martin-cdc42786e4b6)\n",
    "- Python Help: [Inheritance](https://realpython.com/python-interface/), checking [attributes](https://ioflood.com/blog/python-hasattr/), [generic typing](https://stackoverflow.com/questions/63056549/inheriting-from-a-generic-abstract-class-with-a-concrete-type-parameter-is-not-e) for init, unit [testing in jupyter](https://jupyter-tutorial.readthedocs.io/en/stable/notebook/testing/unittest.html) notebooks\n",
    "- Matrices & Normalization:\n",
    "    - [coo](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.coo_matrix.html) & [csr](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html) matrices\n",
    "    - sklearn [normalize](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.normalize.html)\n",
    "- Gotchas:\n",
    "    - Do [not use](https://docs.python.org/3/library/abc.html#abc.abstractproperty) instance variables as validation for ABCs. User getter/setter properties.\n",
    "- ChatGPT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddb7a7ed-e042-46e4-80a4-d72244d9402e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import re\n",
    "import unittest\n",
    "import warnings\n",
    "from abc import ABC, abstractmethod\n",
    "from sys import getsizeof\n",
    "from typing import Any, Dict, Generic, List, Tuple, TypeVar\n",
    "\n",
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix, csr_matrix\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "T = TypeVar(\"T\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7190e02f-c464-4b00-9af6-721f74d5b905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Markov-related Tokenizer Interface. Intended for different data types. (e.g., arrays, strings, text, numbers, etc.)\n",
    "\n",
    "class SequenceTokenizerInterface(ABC, Generic[T]):\n",
    "    @classmethod\n",
    "    def __subclasshook__(cls, subclass):\n",
    "        return (hasattr(subclass, '__init__') and \n",
    "                hasattr(subclass, 'raw') and \n",
    "                hasattr(subclass, 'content') and \n",
    "                hasattr(subclass, 'tokens') and \n",
    "                hasattr(subclass, 'mapping') and \n",
    "                hasattr(subclass, 'tokenize') and \n",
    "                callable(subclass.tokenize) and\n",
    "                hasattr(subclass, 'preprocess') and \n",
    "                callable(subclass.preprocess) and\n",
    "                hasattr(subclass, 'total_tokens') and \n",
    "                callable(subclass.total_tokens) and\n",
    "                hasattr(subclass, 'total_unique_tokens') and \n",
    "                callable(subclass.total_unique_tokens))\n",
    "\n",
    "    @abstractmethod\n",
    "    def __init__(self, content: T) -> None:\n",
    "        \"\"\"Constructor\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def raw(self) -> T:\n",
    "        \"\"\"The raw content\"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def content(self) -> T:\n",
    "        \"\"\"The digested content from raw\"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def tokens(self) -> List[T]:\n",
    "        \"\"\"The tokens from content\"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def mapping(self) -> Dict[T, int]:\n",
    "        \"\"\"The index mappings for unique tokens\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def preprocess(self) -> T:\n",
    "        \"\"\"Preprocess the raw content object before tokenization\"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    @abstractmethod\n",
    "    def tokenize(self) -> Tuple[List[str], Dict[T, int]]:\n",
    "        \"\"\"Tokenize the content object\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def total_tokens(self) -> int:\n",
    "        \"\"\"Get total tokens generated during tokenization\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def total_unique_tokens(self) -> int:\n",
    "        \"\"\"Get total unique tokens generated during tokenization\"\"\"\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "100e4ef7-7f24-433b-bfc1-92e102524e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Lorem', 'ipsum', 'dolor', 'sit', 'amet', ',', 'consectetur', 'adipiscing', 'elit', ',', 'sed', 'do', 'eiusmod', 'tempor', 'incididunt', 'ut', 'labore', 'et', 'dolore', 'magna', 'aliqua', '.', 'Ut', 'enim', 'ad', 'minim', 'veniam', ',', 'quis', 'nostrud', 'exercitation', 'ullamco', 'laboris', 'nisi', 'ut', 'aliquip', 'ex', 'ea', 'commodo', 'consequat', '.', 'Duis', 'aute', 'irure', 'dolor', 'in', 'reprehenderit', 'in', 'voluptate', 'velit', 'esse', 'cillum', 'dolore', 'eu', 'fugiat', 'nulla', 'pariatur', '.', 'Excepteur', 'sint', 'occaecat', 'cupidatat', 'non', 'proident', ',', 'sunt', 'in', 'culpa', 'qui', 'officia', 'deserunt', 'mollit', 'anim', 'id', 'est', 'laborum', '.']\n",
      "{',': 0, '.': 1, 'Duis': 2, 'Excepteur': 3, 'Lorem': 4, 'Ut': 5, 'ad': 6, 'adipiscing': 7, 'aliqua': 8, 'aliquip': 9, 'amet': 10, 'anim': 11, 'aute': 12, 'cillum': 13, 'commodo': 14, 'consectetur': 15, 'consequat': 16, 'culpa': 17, 'cupidatat': 18, 'deserunt': 19, 'do': 20, 'dolor': 21, 'dolore': 22, 'ea': 23, 'eiusmod': 24, 'elit': 25, 'enim': 26, 'esse': 27, 'est': 28, 'et': 29, 'eu': 30, 'ex': 31, 'exercitation': 32, 'fugiat': 33, 'id': 34, 'in': 35, 'incididunt': 36, 'ipsum': 37, 'irure': 38, 'labore': 39, 'laboris': 40, 'laborum': 41, 'magna': 42, 'minim': 43, 'mollit': 44, 'nisi': 45, 'non': 46, 'nostrud': 47, 'nulla': 48, 'occaecat': 49, 'officia': 50, 'pariatur': 51, 'proident': 52, 'qui': 53, 'quis': 54, 'reprehenderit': 55, 'sed': 56, 'sint': 57, 'sit': 58, 'sunt': 59, 'tempor': 60, 'ullamco': 61, 'ut': 62, 'velit': 63, 'veniam': 64, 'voluptate': 65, '<| unknown |>': 66}\n",
      "Total Size: 494b\n",
      "Total Tokens: 77\n",
      "Total Distinct Tokens: 67\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer for long text content (not individual characters in a str)\n",
    "\n",
    "class NLPTextTokens(SequenceTokenizerInterface[str]):\n",
    "\n",
    "    def __init__(self, content: str) -> None:\n",
    "        if not isinstance(content, str):\n",
    "            raise TypeError(\"content is not a string.\")\n",
    "        \n",
    "        self._raw: str = content\n",
    "        self._content: str = self.preprocess()\n",
    "        self._tokens: List[str]\n",
    "        self._mapping: Dict[str, int]\n",
    "        self._tokens, self._mapping = self.tokenize(self._content)\n",
    "\n",
    "    @property\n",
    "    def raw(self) -> str:\n",
    "        return self._raw\n",
    "\n",
    "    @property\n",
    "    def content(self) -> str:\n",
    "        return self._content\n",
    "\n",
    "    @property\n",
    "    def tokens(self) -> List[str]:\n",
    "        return self._tokens\n",
    "    \n",
    "    @property\n",
    "    def mapping(self) -> Dict[str, int]:\n",
    "        return self._mapping\n",
    "    \n",
    "    def preprocess(self) -> str:\n",
    "        punctuation_pad = '!?.,:-;'\n",
    "        punctuation_remove = '\"()_\\n'\n",
    "\n",
    "        content_preprocess = re.sub(r'(\\S)(\\n)(\\S)', r'\\1 \\2 \\3', self._raw)\n",
    "        content_preprocess = content_preprocess.translate(str.maketrans('', '', punctuation_remove))\n",
    "        content_preprocess = content_preprocess.translate(\n",
    "            str.maketrans({k: f' {k} ' for k in punctuation_pad}))\n",
    "        content_preprocess = re.sub(' +', ' ', content_preprocess)\n",
    "        content_preprocess = content_preprocess.strip()\n",
    "\n",
    "        return content_preprocess\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenize(content: List[str]) -> Tuple[List[str], Dict[str, int]]:\n",
    "        content_list = content.split(' ')\n",
    "        content_set = list(set(content_list))\n",
    "        content_set.sort()\n",
    "        content_set.append('<| unknown |>')\n",
    "        content_dict = { v: i for i, v in enumerate(content_set) }\n",
    "\n",
    "        return content_list, content_dict\n",
    "\n",
    "    def total_tokens(self) -> int:\n",
    "        return len(self._tokens) or 0\n",
    "\n",
    "    def total_unique_tokens(self) -> int:\n",
    "        return len(self._mapping.keys()) or 0\n",
    "\n",
    "    def info(self) -> None:\n",
    "        print(f'Total Size: {getsizeof(self.raw)}b\\nTotal Tokens: {self.total_tokens()}\\nTotal Distinct Tokens: {self.total_unique_tokens()}')\n",
    "\n",
    "text = \"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\"\n",
    "t = NLPTextTokens(text)\n",
    "print(t.tokens)\n",
    "print(t.mapping)\n",
    "t.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "834568fc-c46f-4706-8fa2-87dcd453f3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Markov-related State Space\n",
    "\n",
    "class StateSpaceInterface(ABC, Generic[T]):\n",
    "    @classmethod\n",
    "    def __subclasshook__(cls, subclass):\n",
    "        return (hasattr(subclass, 'content') and\n",
    "                hasattr(subclass, 'state_space') and\n",
    "                hasattr(subclass, 'transition_matrix') and\n",
    "                hasattr(subclass, 'transition_probability_matrix') and\n",
    "                hasattr(subclass, 'generate_state_space') and \n",
    "                callable(subclass.generate_state_space) and\n",
    "                hasattr(subclass, 'generate_transition_matrix') and \n",
    "                callable(subclass.generate_transition_matrix) and\n",
    "                hasattr(subclass, 'generate_transition_matrix_prob') and \n",
    "                callable(subclass.generate_transition_matrix_prob))\n",
    "\n",
    "    @abstractmethod\n",
    "    def __init__(self, content: T, *args, **kwargs) -> None:\n",
    "        \"\"\"Constructor\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def content(self) -> T:\n",
    "        \"\"\"The content from which to create a state space\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def state_space(self) -> List[Any]:\n",
    "        \"\"\"The state space items from the content\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def transition_matrix(self) -> coo_matrix:\n",
    "        \"\"\"The transition matrix of the state space\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def transition_probability_matrix(self) -> csr_matrix:\n",
    "        \"\"\"The transition matrix of the state space\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def generate_state_space(self) -> Dict:\n",
    "        \"\"\"Generate the state space\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def generate_transition_matrix(self) -> coo_matrix:\n",
    "        \"\"\"Generate the transition matrix for every state\"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    @abstractmethod\n",
    "    def generate_transition_matrix_prob(self) -> csr_matrix:\n",
    "        \"\"\"Generate the transition matrix for every state\"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74b095a8-45ec-4214-8d0f-e6d3edeefbd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Size: 494b\n",
      "Total Tokens: 77\n",
      "Total Distinct Tokens: 67\n",
      "\n",
      "\n",
      "['Lorem', 'ipsum', 'dolor', 'sit', 'amet', ',', 'consectetur', 'adipiscing', 'elit', ',', 'sed', 'do', 'eiusmod', 'tempor', 'incididunt', 'ut', 'labore', 'et', 'dolore', 'magna', 'aliqua', '.', 'Ut', 'enim', 'ad', 'minim', 'veniam', ',', 'quis', 'nostrud', 'exercitation', 'ullamco', 'laboris', 'nisi', 'ut', 'aliquip', 'ex', 'ea', 'commodo', 'consequat', '.', 'Duis', 'aute', 'irure', 'dolor', 'in', 'reprehenderit', 'in', 'voluptate', 'velit', 'esse', 'cillum', 'dolore', 'eu', 'fugiat', 'nulla', 'pariatur', '.', 'Excepteur', 'sint', 'occaecat', 'cupidatat', 'non', 'proident', ',', 'sunt', 'in', 'culpa', 'qui', 'officia', 'deserunt', 'mollit', 'anim', 'id', 'est', 'laborum', '.'] \n",
      "\n",
      "[4, 37, 21, 58, 10, 0, 15, 7, 25, 0, 56, 20, 24, 60, 36, 62, 39, 29, 22, 42, 8, 1, 5, 26, 6, 43, 64, 0, 54, 47, 32, 61, 40, 45, 62, 9, 31, 23, 14, 16, 1, 2, 12, 38, 21, 35, 55, 35, 65, 63, 27, 13, 22, 30, 33, 48, 51, 1, 3, 57, 49, 18, 46, 52, 0, 59, 35, 17, 53, 50, 19, 44, 11, 34, 28, 41, 1] \n",
      "\n",
      "{',': 0, '.': 1, 'Duis': 2, 'Excepteur': 3, 'Lorem': 4, 'Ut': 5, 'ad': 6, 'adipiscing': 7, 'aliqua': 8, 'aliquip': 9, 'amet': 10, 'anim': 11, 'aute': 12, 'cillum': 13, 'commodo': 14, 'consectetur': 15, 'consequat': 16, 'culpa': 17, 'cupidatat': 18, 'deserunt': 19, 'do': 20, 'dolor': 21, 'dolore': 22, 'ea': 23, 'eiusmod': 24, 'elit': 25, 'enim': 26, 'esse': 27, 'est': 28, 'et': 29, 'eu': 30, 'ex': 31, 'exercitation': 32, 'fugiat': 33, 'id': 34, 'in': 35, 'incididunt': 36, 'ipsum': 37, 'irure': 38, 'labore': 39, 'laboris': 40, 'laborum': 41, 'magna': 42, 'minim': 43, 'mollit': 44, 'nisi': 45, 'non': 46, 'nostrud': 47, 'nulla': 48, 'occaecat': 49, 'officia': 50, 'pariatur': 51, 'proident': 52, 'qui': 53, 'quis': 54, 'reprehenderit': 55, 'sed': 56, 'sint': 57, 'sit': 58, 'sunt': 59, 'tempor': 60, 'ullamco': 61, 'ut': 62, 'velit': 63, 'veniam': 64, 'voluptate': 65, '<| unknown |>': 66} \n",
      "\n",
      "{'consectetur adipiscing elit': 0, 'ea commodo consequat': 1, 'enim ad minim': 2, '. Excepteur sint': 3, 'nostrud exercitation ullamco': 4, 'in culpa qui': 5, ', consectetur adipiscing': 6, 'veniam , quis': 7, 'anim id est': 8, 'amet , consectetur': 9, 'labore et dolore': 10, 'magna aliqua .': 11, 'Ut enim ad': 12, 'ut aliquip ex': 13, 'sed do eiusmod': 14, 'aliqua . Ut': 15, 'velit esse cillum': 16, 'voluptate velit esse': 17, 'fugiat nulla pariatur': 18, 'do eiusmod tempor': 19, 'Lorem ipsum dolor': 20, 'non proident ,': 21, 'quis nostrud exercitation': 22, 'commodo consequat .': 23, 'nulla pariatur .': 24, 'occaecat cupidatat non': 25, 'Excepteur sint occaecat': 26, 'in voluptate velit': 27, 'esse cillum dolore': 28, 'cillum dolore eu': 29, 'adipiscing elit ,': 30, 'laboris nisi ut': 31, 'sint occaecat cupidatat': 32, 'deserunt mollit anim': 33, 'aute irure dolor': 34, 'pariatur . Excepteur': 35, 'eiusmod tempor incididunt': 36, 'eu fugiat nulla': 37, ', sed do': 38, 'est laborum .': 39, 'dolor sit amet': 40, 'minim veniam ,': 41, 'dolore magna aliqua': 42, ', quis nostrud': 43, 'consequat . Duis': 44, 'irure dolor in': 45, 'elit , sed': 46, '. Duis aute': 47, 'dolor in reprehenderit': 48, 'officia deserunt mollit': 49, '. Ut enim': 50, 'id est laborum': 51, 'ut labore et': 52, 'ullamco laboris nisi': 53, 'Duis aute irure': 54, 'nisi ut aliquip': 55, 'tempor incididunt ut': 56, 'in reprehenderit in': 57, 'ipsum dolor sit': 58, 'cupidatat non proident': 59, 'sit amet ,': 60, 'incididunt ut labore': 61, 'et dolore magna': 62, ', sunt in': 63, 'mollit anim id': 64, 'proident , sunt': 65, 'exercitation ullamco laboris': 66, 'dolore eu fugiat': 67, 'aliquip ex ea': 68, 'qui officia deserunt': 69, 'reprehenderit in voluptate': 70, 'ad minim veniam': 71, 'ex ea commodo': 72, 'culpa qui officia': 73, 'sunt in culpa': 74, '<| unknwon |>': 75} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# NLP Text StateSpace\n",
    "\n",
    "class NLPTextStateSpace(StateSpaceInterface[NLPTextTokens]):\n",
    "\n",
    "    @staticmethod\n",
    "    def create_from_text(text: str, n: int = 2):\n",
    "        try:\n",
    "            text_tokens = NLPTextTokens(text)\n",
    "            return NLPTextStateSpace(text_tokens, n)\n",
    "        except Exception:\n",
    "            raise ValueError(\"Exception returned while creating NLP Text Tokenizer\")\n",
    "\n",
    "    def __init__(self, content: NLPTextTokens, n: int = 2) -> None:\n",
    "        if not isinstance(content, NLPTextTokens):\n",
    "            raise TypeError(\"content is not and object of class NLPTextTokens.\")\n",
    "        if not (2 <= n <= 5):\n",
    "            raise ValueError(\"n must be between 2 and 5, inclusive\")\n",
    "\n",
    "        self.n = n\n",
    "        self._content = content\n",
    "        self._n_grams = self.generate_ngrams()\n",
    "        self._state_space = self.generate_state_space()\n",
    "        self._transition_matrix = self.generate_transition_matrix()\n",
    "        self._transition_matrix_prob = self.generate_transition_matrix_prob()\n",
    "\n",
    "    @property\n",
    "    def content(self) -> NLPTextTokens:\n",
    "        return self._content\n",
    "    \n",
    "    @property\n",
    "    def state_space(self) -> List[str]:\n",
    "        return self._state_space\n",
    "    \n",
    "    @property\n",
    "    def transition_matrix(self):\n",
    "        return self._transition_matrix\n",
    "\n",
    "    @property\n",
    "    def transition_probability_matrix(self):\n",
    "        return self._transition_matrix_prob\n",
    "\n",
    "    @property\n",
    "    def ngrams(self):\n",
    "        return self._n_grams\n",
    "\n",
    "    def generate_ngrams(self) -> List:\n",
    "        sequences = [self._content.tokens[i:] for i in range(self.n)]\n",
    "\n",
    "        return [' '.join(ngram) for ngram in list(zip(*sequences))]\n",
    "\n",
    "    def generate_state_space(self) -> Dict[str, int]:\n",
    "        if not self.ngrams:\n",
    "            self.ngrams = self.generate_ngrams()\n",
    "\n",
    "        n_grams_distinct = list(set(self.ngrams))\n",
    "        n_grams_distinct.append(\"<| unknwon |>\")\n",
    "       \n",
    "        return { v: i for i, v in enumerate(n_grams_distinct) }\n",
    "    \n",
    "    def generate_transition_matrix(self) -> coo_matrix:\n",
    "        # create n-gram map if not already done so\n",
    "        if not self.state_space:\n",
    "            self.state_space = self.generate_state_space()\n",
    "\n",
    "        # create coo matrix args\n",
    "        row_ind, col_ind, values = ([] for i in range(3))\n",
    "        for i in range(len(self._content.tokens[:-self.n])):\n",
    "            ngram = ' '.join(self._content.tokens[i:i + self.n])\n",
    "            ngram_ind = self._state_space[ngram]\n",
    "            next_word_ind = self._content.mapping[self._content.tokens[i + self.n]]\n",
    "\n",
    "            row_ind.extend([ngram_ind])\n",
    "            col_ind.extend([next_word_ind])\n",
    "            values.extend([1])\n",
    "\n",
    "        # create and return coo matrix\n",
    "        S = coo_matrix((values, (row_ind, col_ind)), shape=(len(self._state_space), len(self._content.mapping)))\n",
    "        \n",
    "        return S\n",
    "\n",
    "    def generate_transition_matrix_prob(self) -> csr_matrix:\n",
    "        transition_matrix = self.generate_transition_matrix()\n",
    "        \n",
    "        return normalize(transition_matrix, norm='l1', axis=1)\n",
    "\n",
    "ss = NLPTextStateSpace(t, 3)\n",
    "ss.content.info()\n",
    "print('\\n')\n",
    "print(ss.content.tokens, '\\n')\n",
    "print([ss.content.mapping[t] for t in ss.content.tokens], '\\n')\n",
    "print(ss.content.mapping, '\\n')\n",
    "print(ss.state_space, '\\n')\n",
    "# print(ss.transition_matrix, '\\n')\n",
    "# print(ss.transition_probability_matrix, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b001ba4e-885c-41d0-9d9d-8b5d93a4d1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Markov interface to manage process\n",
    "\n",
    "class MarkovChain(ABC, Generic[T]):\n",
    "    \n",
    "    @classmethod\n",
    "    def __subclasshook__(cls, subclass):\n",
    "        return (hasattr(subclass, '__init__') and\n",
    "                hasattr(subclass, 'state_space') and\n",
    "                callable(subclass.check_prefix) and\n",
    "                hasattr(subclass, 'check_prefix') and\n",
    "                callable(subclass.return_next_element) and\n",
    "                hasattr(subclass, 'return_next_element') and\n",
    "                callable(subclass.generate_sequence) and\n",
    "                hasattr(subclass, 'generate_sequence'))\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def state_space(self) -> T:\n",
    "        \"\"\"The content from which to create a state space\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def __init__(self, state_space: T, *args, **kwargs) -> None:\n",
    "        \"\"\"Constructor\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def check_prefix(self, prefix: Any) -> Any:\n",
    "        \"\"\"Checks a prefix against a state space\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def return_next_element(self, prefix: Any) -> Any:\n",
    "        \"\"\"Generates next element of sequence based on a prefix\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def generate_sequence(self, length: int, prefix: Any) -> Any:\n",
    "        \"\"\"Generates a sequence of specified length based on a prefix\"\"\"\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80deafd1-11ae-4dc5-a85c-f8cbd43ca049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "incididunt\n",
      "eiusmod tempor\n",
      "eiusmod tempor incididunt ut labore et dolore magna aliqua . Ut enim ad minim veniam ,\n"
     ]
    }
   ],
   "source": [
    "# Markov chain class to manage NLP Text Sequences\n",
    "\n",
    "class NLPTextMarkovChain(MarkovChain[NLPTextStateSpace]):\n",
    "\n",
    "    def __init__(self, state_space: NLPTextStateSpace):\n",
    "        self._state_space = state_space\n",
    "        self._reverse_word_mapping = { i: v for v, i in self._state_space.content.mapping.items() }\n",
    "\n",
    "    @property\n",
    "    def state_space(self) -> NLPTextStateSpace:\n",
    "        return self._state_space\n",
    "\n",
    "    def random_ngram(self) -> str:\n",
    "        return np.random.choice(self.state_space.ngrams)\n",
    "    \n",
    "    def check_prefix(self, prefix: str) -> str:\n",
    "        prefix_list = prefix.split(' ')[-self._state_space.n:]\n",
    "        if len(prefix_list) < self._state_space.n:\n",
    "            warnings.warn(\n",
    "                f'Prefix is too short, please provide prefix of length: {self._state_space.n}. Random ngram used instead.')\n",
    "            return self.random_ngram()\n",
    "        else:\n",
    "            prefix = ' '.join(prefix_list)\n",
    "            if prefix in self.state_space.ngrams:\n",
    "                return prefix\n",
    "            else:\n",
    "                warnings.warn(\n",
    "                    'Prefix is not included in ngrams of the model. Provide another prefix. Random ngram used instead.')\n",
    "                return self.random_ngram()\n",
    "\n",
    "    def return_next_element(self, prefix: str) -> str:\n",
    "        prefix = self.check_prefix(prefix)\n",
    "        prefix_index = self.state_space.state_space[prefix]\n",
    "        # TODO: add temperature to change weights here\n",
    "        weights = self.state_space.transition_probability_matrix[prefix_index].toarray()[0]\n",
    "        token_index = np.random.choice(range(len(weights)), p=weights)\n",
    "\n",
    "        return self._reverse_word_mapping[token_index]\n",
    "\n",
    "    def generate_sequence(self, length: int, prefix: str) -> str:\n",
    "        prefix = self.check_prefix(prefix)\n",
    "        print(prefix)\n",
    "        sequence = prefix.split(' ')\n",
    "        range_length = length - len(sequence)\n",
    "\n",
    "        for i in range(range_length):\n",
    "            next_word = self.return_next_element(prefix)\n",
    "            sequence.append(next_word)\n",
    "            prefix = ' '.join(sequence[-self._state_space.n:])\n",
    "\n",
    "        return ' '.join(sequence)\n",
    "\n",
    "text = \"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\"\n",
    "ss = NLPTextStateSpace.create_from_text(text, 2)\n",
    "chain = NLPTextMarkovChain(ss)\n",
    "\n",
    "t = 'do eiusmod tempor'\n",
    "print(chain.return_next_element(t))\n",
    "print(chain.generate_sequence(16, t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "083b3548-a6a2-48f8-a045-0665b3095602",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_instantiation (__main__.TestStateSpaces) ... ok\n",
      "test_instantiation (__main__.TestTokenizers) ... ok\n",
      "test_subclass (__main__.TestTokenizers) ... ok\n",
      "test_text_tokens (__main__.TestTokenizers) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.006s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x13efc78b0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing\n",
    "\n",
    "# Tokenizer testing\n",
    "class TestTokenizers(unittest.TestCase):\n",
    "\n",
    "    def setUp(self):\n",
    "        self.text = \"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\"\n",
    "\n",
    "    def test_subclass(self):\n",
    "        self.assertTrue(issubclass(NLPTextTokens, SequenceTokenizerInterface))\n",
    "    \n",
    "    def test_instantiation(self):\n",
    "        # arrange, act, assert\n",
    "        try:\n",
    "            _ = NLPTextTokens(self.text)\n",
    "        except Exception:\n",
    "            self.fail(\"TextTokens() raised Exception unexpectedly!\")\n",
    "\n",
    "        with self.assertRaises(TypeError):\n",
    "            NLPTextTokens(1234)\n",
    "        \n",
    "    def test_text_tokens(self):\n",
    "        # arrange, act\n",
    "        t = NLPTextTokens(self.text)\n",
    "\n",
    "        # assert\n",
    "        self.assertEqual(getsizeof(t.raw), 494)\n",
    "        self.assertEqual(len(t.tokens), 77)\n",
    "        self.assertEqual(len(t.mapping.keys()), 67)\n",
    "        \n",
    "# State Space testing\n",
    "class TestStateSpaces(unittest.TestCase):\n",
    "\n",
    "    def setUp(self):\n",
    "        self.text = \"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\"\n",
    "        self.tokens = NLPTextTokens(self.text)\n",
    "\n",
    "    def test_instantiation(self):\n",
    "        # arrange, act, assert\n",
    "        try:\n",
    "            _ = NLPTextStateSpace(self.tokens)\n",
    "        except Exception:\n",
    "            self.fail(\"TextTokens() raised Exception unexpectedly!\")\n",
    "\n",
    "        with self.assertRaises(TypeError):\n",
    "            NLPTextStateSpace(1234)\n",
    "        with self.assertRaises(ValueError):\n",
    "            NLPTextStateSpace(self.tokens, 6)\n",
    "\n",
    "\n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
